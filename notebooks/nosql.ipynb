{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel=\"stylesheet\" href=\"static/hyrule.css\" type=\"text/css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructured Data and Mapreduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Gain familiarity with NoSQL and nonrelational data\n",
    "* Parsing and aggregating unstructured data through the command line and python\n",
    "* Exposure to the mapreduce framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NoSQL?\n",
    "\n",
    "NoSQL are databases that do not use the traditional table and field based system we are familiar with in SQL databases. Some of the more common NoSQL databases include:\n",
    "\n",
    "* **Key/Value**: simple variable datastore primary used in databases like Redis and Memcached. They are primary used for caching and fast retrieval of small data.\n",
    "* **BSON**: a binary JSON derivitive. Primary used in Mongo and CouchDB. rows become \"documents\" and tables become \"collections\" (in the mongodb syntax)\n",
    "* **Graph**: Uses the idea of a relationship from relational databases to make networks. Most common name is neo4j, use cases are best summarised [here](http://neo4j.com/use-cases/).\n",
    "\n",
    "\n",
    "#### What about data not in a database?\n",
    "Hadoop, for example, is _not_ a database, but a filesystem (HDFS). If we want to think about how hadoop works, we need to step away from databases and think about how we interact with data in files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we generate structure from unstructured data?\n",
    "\n",
    "Imagine we have data coming from logs, like so:\n",
    "\n",
    "```\n",
    "2013-07-22 16:36:13,475 - file - DEBUG - Debug FILE\n",
    "2013-07-22 16:36:13,477 - werkzeug - INFO -  * Running on http://0.0.0.0:5000/\n",
    "```\n",
    "\n",
    "Often there is some hidden structure to the data, it may not be crystal clear or simple. For example, consider using a str split from python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015-03-22', '16:36:13,475', '-', 'file', '-', 'DEBUG', '-', 'Debug', 'FILE']\n",
      "['2015-03-22', '16:36:13,477', '-', 'werkzeug', '-', 'INFO', '-', '*', 'Running', 'on', 'http://0.0.0.0:5000/']\n"
     ]
    }
   ],
   "source": [
    "rows = [\n",
    "    \"\"\"2015-03-22 16:36:13,475 - file - DEBUG - Debug FILE\"\"\",\n",
    "    \"\"\"2015-03-22 16:36:13,477 - werkzeug - INFO -  * Running on http://0.0.0.0:5000/\"\"\",\n",
    "]\n",
    "for row in rows:\n",
    "    print row.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does using str.split() do well, in this case?\n",
    "2. Where does it seem to fall apart?\n",
    "\n",
    "What could be another delimeter for us to split on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015', '03', '22 16:36:13,475 ', ' file ', ' DEBUG ', ' Debug FILE']\n",
      "['2015', '03', '22 16:36:13,477 ', ' werkzeug ', ' INFO ', '  * Running on http://0.0.0.0:5000/']\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    print row.split('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does using the - delimeter improve?\n",
    "2. How could we make this even better?\n",
    "\n",
    "We _could_ continue iterating through to determine what the best \"cleaned up\" version of our transformation would be.  We _should_ also consider reading through the log and determine the pattern:\n",
    "\n",
    "```\n",
    "2015-03-22 16:36:13,475 - file - DEBUG - Debug FILE\n",
    "```\n",
    "\n",
    "Really is modeled to be a base string like this:\n",
    "\n",
    "```\n",
    "datetime - source - log level - log description\n",
    "```\n",
    "\n",
    "We can easily fill in a log format this way to make the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-04-01 16:10:34.870869 - file - DEBUG - Debug FILE\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def log_format(dt, source, level, desc):\n",
    "    # this other form would also work, and be simpler:\n",
    "    #return ' - '.join([str(dt), source, level, desc])\n",
    "    return \"%s - %s - %s - %s\" % (dt, source, level, desc,)\n",
    "\n",
    "print log_format(datetime.datetime.now(), 'file', 'DEBUG', 'Debug FILE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we reverse the pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-03-22 16:36:13,475\n",
      "Debug FILE\n",
      "{'date': '2015-03-22 16:36:13,475', 'source': 'file', 'level': 'DEBUG', 'desc': 'Debug FILE'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# regex compile allows us to name fields and parse logs in more flexible way.\n",
    "# format: (?P<field_name>regex)\n",
    "regex = r'(?P<date>\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2},\\d+) - (?P<source>\\w+) - (?P<level>\\w+) - (?P<desc>[\\w\\W]+)'\n",
    "log_parser = re.compile(regex)\n",
    "m = log_parser.search(rows[0])\n",
    "print m.group('date')\n",
    "print m.group('desc')\n",
    "print m.groupdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out: Mapping Key Values\n",
    "\n",
    "With the following three examples, write a regex string (r'') that creates fields based on groups you define. If your search returns None, then the pattern is not matching. Refer to Sally's tools to practice the concept:\n",
    "\n",
    "[1] should identify the program ran `[cat, sed, sort]` and arguments `[error.log, -n '/etc/']` passed at each pipe (|)\n",
    "```\n",
    "cat error.log | sed -n '/access denied/p' | sort\n",
    "```\n",
    "\n",
    "[2] should identify AJAX call `[POST]`, filename `[congruence1.jpg]`, status call `[200]`\n",
    "```\n",
    "POST /static/img/congruence1.jpg HTTP/1.1 200\n",
    "POST /static/img/team/arnold.png HTTP/1.1 404\n",
    "```\n",
    "\n",
    "[3] should identify gametime [+20 2nd period, +40 3rd period], team `[WPG, NYR]`, who was penalized `[Jiri Tlsusty]`, the penelty `[slashing]`, penalty against `[Carl Hagelin]`.\n",
    "```\n",
    "1ST PERIOD\n",
    "05:20   WPG Jiri Tlusty  Slashing against  Carl Hagelin\n",
    "14:25   NYR Mats Zuccarello  Tripping against  Jim Slater\n",
    "2ND PERIOD\n",
    "09:31   WPG Mathieu Perreault  Interference against  Dominic Moore\n",
    "13:31   NYR Rick Nash  Hooking against  Jacob Trouba\n",
    "3RD PERIOD\n",
    "19:21   WPG Dustin Byfuglien  Slashing against  Mats Zuccarello\n",
    "```\n",
    "**Bonus**: Connect the resulting dictionary into a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating unstructured data\n",
    "Manipulating the unstructured data into a pandas dataframe for data exploration seems intuitive and incredibly useful, but how about when we already know the shape we need, and we're now looking to optimize performance? Let's practice aggregating data as we processed it.\n",
    "\n",
    "We can start with a csv file, which while technically structured, can be aggregated more traditionally:\n",
    "\n",
    "```\n",
    "36,0,3,0,1\n",
    "73,1,3,0,1\n",
    "30,0,3,0,1\n",
    "49,1,3,0,1\n",
    "47,1,11,0,1\n",
    "47,0,11,1,1\n",
    "46,0,5,0,1\n",
    "16,0,3,0,1\n",
    "52,0,4,0,1\n",
    "21,0,3,0,1\n",
    "```\n",
    "\n",
    "We're interested in aggregating two different columns: the first [let's call it age], and the 3rd [let's call it hits]. The pythonist in us says:\n",
    "\n",
    "1. Find the first column\n",
    "2. Make that our \"key\"\n",
    "3. Set or add to a count [1].\n",
    "\n",
    "And likewise for the 3rd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 1\n",
      "46 1\n",
      "47 2\n",
      "30 3\n",
      "36 1\n",
      "52 1\n",
      "73 1\n",
      "1 1\n",
      "0 9\n"
     ]
    }
   ],
   "source": [
    "csvfile = [\n",
    "    '36,0,3,0,1',\n",
    "    '73,1,3,0,1',\n",
    "    '30,0,3,0,1',\n",
    "    '49,1,3,0,1',\n",
    "    '47,1,11,0,1',\n",
    "    '47,0,11,1,1',\n",
    "    '46,0,5,0,1',\n",
    "    '30,0,3,0,1',\n",
    "    '52,0,4,0,1',\n",
    "    '30,0,3,0,1',\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "age = defaultdict(int)\n",
    "hits = defaultdict(int)\n",
    "for row in csvfile:\n",
    "    a, b, c, d, e = row.split(',')\n",
    "    age[a] += 1\n",
    "    hits[d] += 1\n",
    "\n",
    "for k,v in age.items():\n",
    "    print k, v\n",
    "    \n",
    "for k,v in hits.items():\n",
    "    print k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know a faster interpretation would be pandas, but instead, consider this counter as a _task_. And if we are not working with structured data, putting the structure into pandas and then shaping the summary is an additional task.\n",
    "\n",
    "What if we were working with ipython logs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'I': 6, 'W': 5})\n",
      "defaultdict(<type 'int'>, {'10:18 W': 2, '10:17 I': 5, '10:18 I': 1, '10:17 W': 3})\n"
     ]
    }
   ],
   "source": [
    "ipythonlogs = [\n",
    "    \"\"\"[I 10:17:42.633 NotebookApp] Using MathJax from CDN: https://cdn.mathjax.org/mathjax/latest/MathJax.js\"\"\",\n",
    "    \"\"\"[W 10:17:42.670 NotebookApp] Terminals not available (error was No module named terminado)\"\"\",\n",
    "    \"\"\"[I 10:17:42.670 NotebookApp] Serving notebooks from local directory: /Users/macbook/projects/\"\"\",\n",
    "    \"\"\"[I 10:17:42.670 NotebookApp] 0 active kernels\"\"\",\n",
    "    \"\"\"[I 10:17:42.670 NotebookApp] The IPython Notebook is running at: http://localhost:8888/\"\"\",\n",
    "    \"\"\"[I 10:17:42.670 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\"\"\",\n",
    "    \"\"\"[W 10:17:51.036 NotebookApp] 404 GET /api/kernels/ff97dfc7-e80a-49a5-9064-6d68d4fdbeb5/channels?session_id=7B3E0E7D6F024DA4A43433953CAF3B73 (::1): Kernel does not exist: ff97dfc7-e80a-49a5-9064-6d68d4fdbeb5\"\"\",\n",
    "    \"\"\"[W 10:17:51.051 NotebookApp] 404 GET /api/kernels/ff97dfc7-e80a-49a5-9064-6d68d4fdbeb5/channels?session_id=7B3E0E7D6F024DA4A43433953CAF3B73 (::1) 17.13ms referer=None\"\"\",\n",
    "    \"\"\"[I 10:18:09.405 NotebookApp] Kernel started: 82092ca9-abb3-4196-a967-0694c8a3bec4\"\"\",\n",
    "    \"\"\"[W 10:18:56.046 NotebookApp] 404 GET /api/kernels/ff97dfc7-e80a-49a5-9064-6d68d4fdbeb5/channels?session_id=7B3E0E7D6F024DA4A43433953CAF3B73 (::1): Kernel does not exist: ff97dfc7-e80a-49a5-9064-6d68d4fdbeb5\"\"\",\n",
    "    \"\"\"[W 10:18:56.047 NotebookApp] 404 GET /api/kernels/ff97dfc7-e80a-49a5-9064-6d68d4fdbeb5/channels?session_id=7B3E0E7D6F024DA4A43433953CAF3B73 (::1) 2.11ms referer=None\"\"\",\n",
    "]\n",
    "### aggregating the info [i] vs warning [w] should be relatively straightforward:\n",
    "loggertypes = defaultdict(int)\n",
    "for row in ipythonlogs:\n",
    "    logtype = row[1]\n",
    "    loggertypes[logtype] += 1\n",
    "\n",
    "print loggertypes\n",
    "\n",
    "### How do we aggregate types by minute?\n",
    "loggertypes = defaultdict(int)\n",
    "for row in ipythonlogs:\n",
    "    logtype = row[1:8]\n",
    "    # \"sort\" by time instead, so let's reorder the key:\n",
    "    logtype = ' '.join([logtype.split()[1],logtype.split()[0]]) \n",
    "    loggertypes[logtype] += 1\n",
    "\n",
    "print loggertypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it out: Aggregating Key Values\n",
    "\n",
    "[1] Work through a larger subset of penalties to aggregate\n",
    "\n",
    "1. penalties by team\n",
    "2. penalties by player (either who did it or who it targeted)\n",
    "3. penalties by period\n",
    "\n",
    "```\n",
    "1ST PERIOD\n",
    "02:16   ANA Tim Jackman  Fighting (maj) against  John Scott\n",
    "02:16   SJS John Scott  Fighting (maj) against  Tim Jackman\n",
    "12:53   SJS Adam Burish  Slashing against  Tim Jackman\n",
    "15:38   SJS Matt Nieto  Hooking against  Matt Beleskey\n",
    "18:38   ANA Tim Jackman  Hooking against  Tommy Wingels\n",
    "2ND PERIOD\n",
    "04:35   SJS Justin Braun  Holding against  Jakob Silfverberg\n",
    "09:40   SJS Scott Hannan  Hi-sticking against  Sami Vatanen\n",
    "09:40   ANA Sami Vatanen  Embellishment against  Scott Hannan\n",
    "09:52   SJS Mirco Mueller  Unsportsmanlike conduct against  Ryan Getzlaf\n",
    "09:52   ANA Ryan Getzlaf  Unsportsmanlike conduct against  Mirco Mueller\n",
    "15:23   SJS Adam Burish  Delay Gm - Face-off Violation\n",
    "18:57   SJS Tommy Wingels  Fighting (maj) against  Corey Perry\n",
    "18:57   ANA Corey Perry  Fighting (maj) against  Tommy Wingels\n",
    "3RD PERIOD\n",
    "07:38   ANA Sami Vatanen  Holding against  Tommy Wingels\n",
    "10:18   SJS Joe Pavelski  Fighting (maj) against  Ben Lovejoy\n",
    "10:18   ANA Ben Lovejoy  Fighting (maj) against  Joe Pavelski\n",
    "10:18   ANA Tim Jackman  Roughing against  Marc-Edouard Vlasic\n",
    "10:18   ANA Tim Jackman  Roughing against  Marc-Edouard Vlasic\n",
    "12:58   SJS Justin Braun  Misconduct (10 min) against  Corey Perry\n",
    "12:58   ANA Corey Perry  Misconduct (10 min) against  Justin Braun\n",
    "12:58   ANA Corey Perry  Roughing against  Justin Braun\n",
    "12:58   SJS Justin Braun  Roughing against  Corey Perry\n",
    "13:56   SJS Adam Burish  Roughing against  Nate Thompson\n",
    "13:56   SJS Adam Burish  Misconduct (10 min)\n",
    "13:56   SJS John Scott  Game misconduct\n",
    "13:56   SJS John Scott  Fighting (maj) against  Tim Jackman\n",
    "13:56   SJS John Scott  Instigator against  Tim Jackman\n",
    "13:56   SJS John Scott  Player leaves bench - bench against  Hampus Lindholm\n",
    "13:56   ANA Nate Thompson  Misconduct (10 min)\n",
    "13:56   ANA Nate Thompson  Roughing against  Adam Burish\n",
    "13:56   ANA Nate Thompson  Roughing against  Adam Burish\n",
    "13:56   ANA Tim Jackman  Misconduct (10 min) against  John Scott\n",
    "16:54   ANA Matt Beleskey  Misconduct (10 min)\n",
    "16:54   ANA William Karlsson  Slashing against  Marc-Edouard Vlasic\n",
    "16:54   ANA Ryan Getzlaf  Fighting (maj) against  James Sheppard\n",
    "16:54   SJS James Sheppard  Fighting (maj) against  Ryan Getzlaf\n",
    "16:54   ANA Ryan Kesler  Misconduct (10 min)\n",
    "```\n",
    "\n",
    "[2] In sample.access.log, filter down to the 404s and aggregate by day the full url and client. Example below:\n",
    "\n",
    "url: `/tag/dork/`  \n",
    "client: `Mozilla/5.0 (compatible; Ezooms/1.0; ezooms.bot@gmail.com)`\n",
    "```\n",
    "127.0.0.1 - - [05/Dec/2011:16:45:39 -0500] \"GET /tag/dork/ HTTP/1.0\" 404 29262 \"-\" \"Mozilla/5.0 (compatible; Ezooms/1.0; ezooms.bot@gmail.com)\"\n",
    "```\n",
    "\n",
    "For question 2, let's use python [standard input](http://stackoverflow.com/questions/1450393/how-do-you-read-from-stdin-in-python) and save it as a script so it should work like below, printing to standard out.\n",
    "\n",
    "```sh\n",
    "cat sample.access.log | log_agg.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is mapreduce?\n",
    "Mapreduce is a concept of splitting work across a system of small computers, but ultimately, explains processing in the following steps:\n",
    "\n",
    "1. Map: Produce key|value pairs depending on the task\n",
    "2. Shuffle|Sort: Sorts the key|value pairs\n",
    "3. Reduce: Combines pairs into a single output.\n",
    "\n",
    "Ideally this follows the pattern across a distributed network of computers:\n",
    "\n",
    "1. Mappers: computers tasked to take in data and run a map task\n",
    "2. Sorters: computers tasked to take the data from the mappers and sort\n",
    "3. Reducers: computers tasked to take in the sorted data and reduce the information\n",
    "\n",
    "<img src='img/mapreduce.jpg' width='800'>\n",
    "\n",
    "The three tasks are technically synchronous. This means that the computers will not start reducing until all mappers have completed their jobs. It's important for the processor or system in place to do its best to split the data processing evenly: if you have one mapper doing all the work, there is no advantage!\n",
    "\n",
    "Tasks also also (typically) solved linearly. That is, we want to limit the number of passes through the data. So , consider how we could solve each of these reducer tasks:\n",
    "\n",
    "`count`: We've solved this already! Take the previous count and add to it. mapreduce will want to presort keys so it is not searching for keys on the fly. What is something that could be solved similar to count?\n",
    "\n",
    "`max or min`: How would we solve these?\n",
    "\n",
    "Where would machine learning make sense in map reduce? Where would it not?\n",
    "\n",
    "#### Sidenote: What is MapReduce?\n",
    "MapReduce is Google's internal platform, though it has since been generalized.\n",
    "\n",
    "#### How can we reproduce this behavior on our own computers?\n",
    "There's two relatively straight forward ways we could consider emulating mapreduce:\n",
    "\n",
    "1. Run python scripts in shell with the shell sort command to get a sense of how data is funnelled:\n",
    "    ```sh\n",
    "    cat data.txt | mapper.py | sort | reducer.py\n",
    "    ```\n",
    "2. Run a python script with multiprocessing pools to see how processing can be expedited with multiple core computers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mapper example\n",
    "\n",
    "A mapper should take in a queue of data, and then spit out a queue of data. For passing data between functions, we'll use a list of tuples (since they are immutable); otherwise we'd use standard out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('quick', 1), ('brown', 1), ('fox', 1), ('jumped', 1), ('over', 1), ('the', 1), ('lazy', 1), ('dog', 1)]\n"
     ]
    }
   ],
   "source": [
    "def mapper(line):\n",
    "    result = []\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # remove odd symbols from the text\n",
    "    line = re.sub('[!\"§$%&/()=?*#()\\[\\],.<>:;~_-]',\"\", line)\n",
    "    # split the line into words\n",
    "    words = line.split(\" \")\n",
    "    # insert the cleaned words into the results list\n",
    "    for word in words:\n",
    "        result.append((word, 1))\n",
    "    # output is a list of (key, value) pairs\n",
    "    return result\n",
    "\n",
    "print mapper('the quick brown fox jumped over the lazy dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reducer example\n",
    "Our reducer takes in a key and list of values to reduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reducer(key, values):\n",
    "    return key, sum(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle runs very similarly to how we've done aggregations before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(words, print_shuffle=False):\n",
    "    tmp = \"\"\n",
    "    val_list = []\n",
    "    for i in words:\n",
    "        if tmp and i[0] != tmp:\n",
    "            if print_shuffle:\n",
    "                print tmp, val_list\n",
    "            print reducer(tmp,val_list)\n",
    "            val_list=[]\n",
    "        tmp = i[0]\n",
    "        val_list.append(i[1])\n",
    "    # Don't forget to print out the last key value pair!\n",
    "    if print_shuffle:\n",
    "        print tmp, val_list\n",
    "    print reducer(tmp,val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('brown', 1)\n",
      "('dog', 1)\n",
      "('fox', 1)\n",
      "('jumped', 1)\n",
      "('lazy', 1)\n",
      "('over', 1)\n",
      "('quick', 1)\n",
      "('the', 2)\n"
     ]
    }
   ],
   "source": [
    "# while this looks \"backwards,\" mapper is called first,\n",
    "# then we sort,\n",
    "#then shuffle runs the reducer.\n",
    "shuffle(sorted(mapper('the quick brown fox jumped over the lazy dog')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view another example by echoing or cat text into the mapper.py file, sorting, and then running reducer.py:\n",
    "\n",
    "```sh\n",
    "echo 'big big data big data science!' | python mapper.py | sort | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, our example using multiprocessing for a map function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('big', 2)\n",
      "('data', 1)\n",
      "('big', 1)\n",
      "('data', 1)\n",
      "('science!', 1)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def word_mapper(word):\n",
    "    return (word, 1)\n",
    "\n",
    "line = 'big big data big data science!'\n",
    "words = line.split()\n",
    "pool = multiprocessing.Pool(len(words))\n",
    "\n",
    "mapped_words = pool.map(word_mapper, words)\n",
    "pool.terminate()\n",
    "\n",
    "shuffle(mapped_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice \n",
    "\n",
    "Pick one of the larger tasks above (either the long penalty list or the access log) and practice the mapper | reducer split. For the most part, this should be taking your code apart a bit and determining what you are mapping into key value pairs, and how you are reducing (the reducing will probably be very similar, if not exactly, to the above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review, Reading, Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on multiprocessing in python\n",
    "* [mapreduce with python multiprocessing](https://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/)\n",
    "* [parallelization in ipython](http://ipython.org/ipython-doc/dev/parallel/)\n",
    "\n",
    "#### Hadoop\n",
    "For those who want to further engage themselves and see how a _basic_ hadoop system works, consider:\n",
    "\n",
    "* Reading through the two tutorials on [setting up a hadoop cluster](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/) and [running python scripts in hadoop](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/). Consider using a virtual machine to keep everything tidy so it does not mess with your computer!\n",
    "* [Hortonworks](http://hortonworks.com/tutorials/#tuts-analysts) provides a variety of tutorials for data analysts and scientists to learn hadoop and hadoop's scripting languages, and they have a sandbox virtual machine you can practice on.\n",
    "* This [Spark](http://lintool.github.io/SparkTutorial/) tutorial can start you in the more \"new\" direction, while Spark currently remains hot technology.\n",
    "\n",
    "Additional reading:\n",
    "\n",
    "* [Forbes: Is it Time for Hadoop Alternatives?](http://www.forbes.com/sites/johnwebster/2014/12/08/is-it-time-for-hadoop-alternatives/)\n",
    "* [IBM: What is MapReduce?](http://www-01.ibm.com/software/data/infosphere/hadoop/mapreduce/)\n",
    "* [Wakari MapReduce IPython notebook](https://www.wakari.io/sharing/bundle/nkorf/MapReduce%20Example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
